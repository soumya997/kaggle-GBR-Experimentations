{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a47c34f",
   "metadata": {
    "papermill": {
     "duration": 0.028238,
     "end_time": "2022-02-11T23:13:52.285132",
     "exception": false,
     "start_time": "2022-02-11T23:13:52.256894",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Yolov5 high resolution training\n",
    "\n",
    "### Major modification\n",
    "* img=3600\n",
    "* mixup=0.5\n",
    "* fliplr: 0.5\n",
    "\n",
    "### Hardware to reproduce\n",
    "* RTX3090"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c6aea0",
   "metadata": {
    "papermill": {
     "duration": 0.025663,
     "end_time": "2022-02-11T23:13:52.337411",
     "exception": false,
     "start_time": "2022-02-11T23:13:52.311748",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Training Log:\n",
    "> ```\n",
    "version=1\n",
    "img_size:3584,bs2,e11,[yolov5s6] \n",
    "Fold: video2[validation]\n",
    "Labels: only GT\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72cde8b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-11T23:13:52.401581Z",
     "iopub.status.busy": "2022-02-11T23:13:52.399862Z",
     "iopub.status.idle": "2022-02-11T23:13:53.135176Z",
     "shell.execute_reply": "2022-02-11T23:13:53.134522Z",
     "shell.execute_reply.started": "2022-02-11T22:44:05.669662Z"
    },
    "papermill": {
     "duration": 0.770177,
     "end_time": "2022-02-11T23:13:53.135339",
     "exception": false,
     "start_time": "2022-02-11T23:13:52.365162",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Feb 11 23:13:53 2022       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 450.119.04   Driver Version: 450.119.04   CUDA Version: 11.0     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\r\n",
      "| N/A   37C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b24e9126",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-02-11T23:13:53.191239Z",
     "iopub.status.busy": "2022-02-11T23:13:53.190480Z",
     "iopub.status.idle": "2022-02-11T23:13:55.755764Z",
     "shell.execute_reply": "2022-02-11T23:13:55.755179Z",
     "shell.execute_reply.started": "2022-02-11T22:44:06.391324Z"
    },
    "papermill": {
     "duration": 2.59585,
     "end_time": "2022-02-11T23:13:55.755918",
     "exception": false,
     "start_time": "2022-02-11T23:13:53.160068",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from shutil import copyfile\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import ast\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import importlib\n",
    "import cv2 \n",
    "\n",
    "import shutil\n",
    "from shutil import copyfile, make_archive\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from PIL import Image\n",
    "from string import Template\n",
    "from IPython.display import display\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "TRAIN_PATH = '/kaggle/input/tensorflow-great-barrier-reef'\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f552f27d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-11T23:13:55.808815Z",
     "iopub.status.busy": "2022-02-11T23:13:55.807255Z",
     "iopub.status.idle": "2022-02-11T23:13:55.809427Z",
     "shell.execute_reply": "2022-02-11T23:13:55.809858Z",
     "shell.execute_reply.started": "2022-02-11T22:44:08.820800Z"
    },
    "papermill": {
     "duration": 0.030343,
     "end_time": "2022-02-11T23:13:55.809994",
     "exception": false,
     "start_time": "2022-02-11T23:13:55.779651",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train = pd.read_csv('../input/tensorflow-great-barrier-reef/train.csv')\n",
    "# train['pos'] = train.annotations != '[]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ff8a58a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-11T23:13:55.864747Z",
     "iopub.status.busy": "2022-02-11T23:13:55.863848Z",
     "iopub.status.idle": "2022-02-11T23:13:58.419078Z",
     "shell.execute_reply": "2022-02-11T23:13:58.418543Z",
     "shell.execute_reply.started": "2022-02-11T22:44:08.826960Z"
    },
    "papermill": {
     "duration": 2.585566,
     "end_time": "2022-02-11T23:13:58.419202",
     "exception": false,
     "start_time": "2022-02-11T23:13:55.833636",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'yolov5-w-f2-mod'...\r\n",
      "remote: Enumerating objects: 132, done.\u001b[K\r\n",
      "remote: Counting objects: 100% (132/132), done.\u001b[K\r\n",
      "remote: Compressing objects: 100% (105/105), done.\u001b[K\r\n",
      "remote: Total 132 (delta 36), reused 120 (delta 26), pack-reused 0\u001b[K\r\n",
      "Receiving objects: 100% (132/132), 1.39 MiB | 4.70 MiB/s, done.\r\n",
      "Resolving deltas: 100% (36/36), done.\r\n"
     ]
    }
   ],
   "source": [
    "# !git clone https://github.com/ultralytics/yolov5.git\n",
    "!git clone https://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@github.com/soumya997/yolov5-w-f2-mod.git\n",
    "!mv ./yolov5-w-f2-mod ./yolov5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ef85ded",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-11T23:13:58.479520Z",
     "iopub.status.busy": "2022-02-11T23:13:58.478712Z",
     "iopub.status.idle": "2022-02-11T23:13:58.480796Z",
     "shell.execute_reply": "2022-02-11T23:13:58.481265Z",
     "shell.execute_reply.started": "2022-02-11T22:44:11.692708Z"
    },
    "papermill": {
     "duration": 0.034752,
     "end_time": "2022-02-11T23:13:58.481402",
     "exception": false,
     "start_time": "2022-02-11T23:13:58.446650",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Specify your custom set of Albumentations here\n",
    "# See https://www.kaggle.com/alexchwong/cots-albumentations-gallery for some ideas on what to include\n",
    "# Ensure there are at least 16 spaces after each new line, for correct indentations consistent with yolov5's augmentations.py script\n",
    "\n",
    "################ 16 hashes here\n",
    "\n",
    "# ALBUMENTATIONS = '''[\n",
    "#                 # Add your Albumentations after this line\n",
    "#                     A.OneOf([\n",
    "#                         A.MotionBlur(p=.2),\n",
    "#                         A.MedianBlur(blur_limit=3, p=0.3),\n",
    "#                         A.Blur(blur_limit=3, p=0.1),\n",
    "#                     ], p=0.3),\n",
    "#                     A.OneOf([\n",
    "#                         A.CLAHE(clip_limit=2),\n",
    "#                         A.RandomBrightnessContrast(),            \n",
    "#                     ], p=0.3),\n",
    "#                 # Do not edit past this line\n",
    "#                 ],\n",
    "# # '''\n",
    "# ALBUMENTATIONS = '''[\n",
    "#                 # Add your Albumentations after this line\n",
    "#                     A.OneOf([\n",
    "#                         A.HueSaturationValue(hue_shift_limit = 0.2, \n",
    "#                                              sat_shift_limit = 0.2,\n",
    "#                                              val_shift_limit = 0.2,\n",
    "#                                              p = 0.3), \n",
    "\n",
    "#                         A.RandomBrightnessContrast(brightness_limit = 0.2,                                             \n",
    "#                                                    contrast_limit = 0.2,\n",
    "#                                                    p = 0.3),\n",
    "#                         A.RGBShift(r_shift_limit = 20/255, \n",
    "#                                    g_shift_limit = 20/255, \n",
    "#                                    b_shift_limit = 10/255,\n",
    "#                                    p = 0.3)\n",
    "#                     ], \n",
    "#                     p = 0.2),\n",
    "#                 A.OneOf([\n",
    "#                             A.RandomGamma(gamma_limit = (80, 120),p = 0.3),\n",
    "#                             A.Blur(p = 0.6),\n",
    "#                             A.GaussNoise(var_limit = (0.01, 0.05), mean = 0, p = 0.05),\n",
    "#                             A.ToGray(p = 0.05),\n",
    "#                             A.Solarize(p=0.3)\n",
    "\n",
    "#                         ],\n",
    "#                         p = 0.1),\n",
    "#                 A.ToGray(p=0.01),\n",
    "#                 A.RandomFog(fog_coef_lower = 0.1,\n",
    "#                                 fog_coef_upper = 0.2,\n",
    "#                                 p = 0.03),          \n",
    "#                 # Do not edit past this line\n",
    "#                 ],\n",
    "# '''\n",
    "\n",
    "################ If you are not convinced whether your Albumentations are working, uncomment the following line to make all the training pictures gray at the end:\n",
    "\n",
    "ALBUMENTATIONS = \"[A.ToGray(p=1.0),],\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a6531ea",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2022-02-11T23:13:58.552263Z",
     "iopub.status.busy": "2022-02-11T23:13:58.536416Z",
     "iopub.status.idle": "2022-02-11T23:13:59.233823Z",
     "shell.execute_reply": "2022-02-11T23:13:59.232550Z",
     "shell.execute_reply.started": "2022-02-11T22:44:11.700088Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "papermill": {
     "duration": 0.726545,
     "end_time": "2022-02-11T23:13:59.233961",
     "exception": false,
     "start_time": "2022-02-11T23:13:58.507416",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "# YOLOv5 ðŸš€ by Ultralytics, GPL-3.0 license\r\n",
      "\"\"\"\r\n",
      "Image augmentation functions\r\n",
      "\"\"\"\r\n",
      "\r\n",
      "import math\r\n",
      "import random\r\n",
      "\r\n",
      "import cv2\r\n",
      "import numpy as np\r\n",
      "\r\n",
      "from utils.general import LOGGER, check_version, colorstr, resample_segments, segment2box\r\n",
      "from utils.metrics import bbox_ioa\r\n",
      "\r\n",
      "\r\n",
      "class Albumentations:\r\n",
      "    # YOLOv5 Albumentations class (optional, only used if package is installed)\r\n",
      "    def __init__(self):\r\n",
      "        self.transform = None\r\n",
      "        try:\r\n",
      "            import albumentations as A\r\n",
      "            # check_version(A.__version__, '1.0.3', hard=True)  # version requirement\r\n",
      "\r\n",
      "            self.transform = A.Compose([A.ToGray(p=1.0),],\r\n",
      "                bbox_params=A.BboxParams(format='yolo', label_fields=['class_labels']))\r\n",
      "\r\n",
      "            LOGGER.info(colorstr('albumentations: ') + ', '.join(f'{x}' for x in self.transform.transforms if x.p))\r\n",
      "        except ImportError:  # package not installed, skip\r\n",
      "            pass\r\n",
      "        except Exception as e:\r\n",
      "            LOGGER.info(colorstr('albumentations: ') + f'{e}')\r\n",
      "\r\n",
      "    def __call__(self, im, labels, p=1.0):\r\n",
      "        if self.transform and random.random() < p:\r\n",
      "            new = self.transform(image=im, bboxes=labels[:, 1:], class_labels=labels[:, 0])  # transformed\r\n",
      "            im, labels = new['image'], np.array([[c, *b] for c, b in zip(new['class_labels'], new['bboxes'])])\r\n",
      "        return im, labels\r\n",
      "\r\n",
      "\r\n",
      "def augment_hsv(im, hgain=0.5, sgain=0.5, vgain=0.5):\r\n",
      "    # HSV color-space augmentation\r\n",
      "    if hgain or sgain or vgain:\r\n",
      "        r = np.random.uniform(-1, 1, 3) * [hgain, sgain, vgain] + 1  # random gains\r\n",
      "        hue, sat, val = cv2.split(cv2.cvtColor(im, cv2.COLOR_BGR2HSV))\r\n",
      "        dtype = im.dtype  # uint8\r\n",
      "\r\n",
      "        x = np.arange(0, 256, dtype=r.dtype)\r\n",
      "        lut_hue = ((x * r[0]) % 180).astype(dtype)\r\n",
      "        lut_sat = np.clip(x * r[1], 0, 255).astype(dtype)\r\n",
      "        lut_val = np.clip(x * r[2], 0, 255).astype(dtype)\r\n",
      "\r\n",
      "        im_hsv = cv2.merge((cv2.LUT(hue, lut_hue), cv2.LUT(sat, lut_sat), cv2.LUT(val, lut_val)))\r\n",
      "        cv2.cvtColor(im_hsv, cv2.COLOR_HSV2BGR, dst=im)  # no return needed\r\n",
      "\r\n",
      "\r\n",
      "def hist_equalize(im, clahe=True, bgr=False):\r\n",
      "    # Equalize histogram on BGR image 'im' with im.shape(n,m,3) and range 0-255\r\n",
      "    yuv = cv2.cvtColor(im, cv2.COLOR_BGR2YUV if bgr else cv2.COLOR_RGB2YUV)\r\n",
      "    if clahe:\r\n",
      "        c = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\r\n",
      "        yuv[:, :, 0] = c.apply(yuv[:, :, 0])\r\n",
      "    else:\r\n",
      "        yuv[:, :, 0] = cv2.equalizeHist(yuv[:, :, 0])  # equalize Y channel histogram\r\n",
      "    return cv2.cvtColor(yuv, cv2.COLOR_YUV2BGR if bgr else cv2.COLOR_YUV2RGB)  # convert YUV image to RGB\r\n",
      "\r\n",
      "\r\n",
      "def replicate(im, labels):\r\n",
      "    # Replicate labels\r\n",
      "    h, w = im.shape[:2]\r\n",
      "    boxes = labels[:, 1:].astype(int)\r\n",
      "    x1, y1, x2, y2 = boxes.T\r\n",
      "    s = ((x2 - x1) + (y2 - y1)) / 2  # side length (pixels)\r\n",
      "    for i in s.argsort()[:round(s.size * 0.5)]:  # smallest indices\r\n",
      "        x1b, y1b, x2b, y2b = boxes[i]\r\n",
      "        bh, bw = y2b - y1b, x2b - x1b\r\n",
      "        yc, xc = int(random.uniform(0, h - bh)), int(random.uniform(0, w - bw))  # offset x, y\r\n",
      "        x1a, y1a, x2a, y2a = [xc, yc, xc + bw, yc + bh]\r\n",
      "        im[y1a:y2a, x1a:x2a] = im[y1b:y2b, x1b:x2b]  # im4[ymin:ymax, xmin:xmax]\r\n",
      "        labels = np.append(labels, [[labels[i, 0], x1a, y1a, x2a, y2a]], axis=0)\r\n",
      "\r\n",
      "    return im, labels\r\n",
      "\r\n",
      "\r\n",
      "def letterbox(im, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True, stride=32):\r\n",
      "    # Resize and pad image while meeting stride-multiple constraints\r\n",
      "    shape = im.shape[:2]  # current shape [height, width]\r\n",
      "    if isinstance(new_shape, int):\r\n",
      "        new_shape = (new_shape, new_shape)\r\n",
      "\r\n",
      "    # Scale ratio (new / old)\r\n",
      "    r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\r\n",
      "    if not scaleup:  # only scale down, do not scale up (for better val mAP)\r\n",
      "        r = min(r, 1.0)\r\n",
      "\r\n",
      "    # Compute padding\r\n",
      "    ratio = r, r  # width, height ratios\r\n",
      "    new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\r\n",
      "    dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding\r\n",
      "    if auto:  # minimum rectangle\r\n",
      "        dw, dh = np.mod(dw, stride), np.mod(dh, stride)  # wh padding\r\n",
      "    elif scaleFill:  # stretch\r\n",
      "        dw, dh = 0.0, 0.0\r\n",
      "        new_unpad = (new_shape[1], new_shape[0])\r\n",
      "        ratio = new_shape[1] / shape[1], new_shape[0] / shape[0]  # width, height ratios\r\n",
      "\r\n",
      "    dw /= 2  # divide padding into 2 sides\r\n",
      "    dh /= 2\r\n",
      "\r\n",
      "    if shape[::-1] != new_unpad:  # resize\r\n",
      "        im = cv2.resize(im, new_unpad, interpolation=cv2.INTER_LINEAR)\r\n",
      "    top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))\r\n",
      "    left, right = int(round(dw - 0.1)), int(round(dw + 0.1))\r\n",
      "    im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)  # add border\r\n",
      "    return im, ratio, (dw, dh)\r\n",
      "\r\n",
      "\r\n",
      "def random_perspective(im, targets=(), segments=(), degrees=10, translate=.1, scale=.1, shear=10, perspective=0.0,\r\n",
      "                       border=(0, 0)):\r\n",
      "    # torchvision.transforms.RandomAffine(degrees=(-10, 10), translate=(0.1, 0.1), scale=(0.9, 1.1), shear=(-10, 10))\r\n",
      "    # targets = [cls, xyxy]\r\n",
      "\r\n",
      "    height = im.shape[0] + border[0] * 2  # shape(h,w,c)\r\n",
      "    width = im.shape[1] + border[1] * 2\r\n",
      "\r\n",
      "    # Center\r\n",
      "    C = np.eye(3)\r\n",
      "    C[0, 2] = -im.shape[1] / 2  # x translation (pixels)\r\n",
      "    C[1, 2] = -im.shape[0] / 2  # y translation (pixels)\r\n",
      "\r\n",
      "    # Perspective\r\n",
      "    P = np.eye(3)\r\n",
      "    P[2, 0] = random.uniform(-perspective, perspective)  # x perspective (about y)\r\n",
      "    P[2, 1] = random.uniform(-perspective, perspective)  # y perspective (about x)\r\n",
      "\r\n",
      "    # Rotation and Scale\r\n",
      "    R = np.eye(3)\r\n",
      "    a = random.uniform(-degrees, degrees)\r\n",
      "    # a += random.choice([-180, -90, 0, 90])  # add 90deg rotations to small rotations\r\n",
      "    s = random.uniform(1 - scale, 1 + scale)\r\n",
      "    # s = 2 ** random.uniform(-scale, scale)\r\n",
      "    R[:2] = cv2.getRotationMatrix2D(angle=a, center=(0, 0), scale=s)\r\n",
      "\r\n",
      "    # Shear\r\n",
      "    S = np.eye(3)\r\n",
      "    S[0, 1] = math.tan(random.uniform(-shear, shear) * math.pi / 180)  # x shear (deg)\r\n",
      "    S[1, 0] = math.tan(random.uniform(-shear, shear) * math.pi / 180)  # y shear (deg)\r\n",
      "\r\n",
      "    # Translation\r\n",
      "    T = np.eye(3)\r\n",
      "    T[0, 2] = random.uniform(0.5 - translate, 0.5 + translate) * width  # x translation (pixels)\r\n",
      "    T[1, 2] = random.uniform(0.5 - translate, 0.5 + translate) * height  # y translation (pixels)\r\n",
      "\r\n",
      "    # Combined rotation matrix\r\n",
      "    M = T @ S @ R @ P @ C  # order of operations (right to left) is IMPORTANT\r\n",
      "    if (border[0] != 0) or (border[1] != 0) or (M != np.eye(3)).any():  # image changed\r\n",
      "        if perspective:\r\n",
      "            im = cv2.warpPerspective(im, M, dsize=(width, height), borderValue=(114, 114, 114))\r\n",
      "        else:  # affine\r\n",
      "            im = cv2.warpAffine(im, M[:2], dsize=(width, height), borderValue=(114, 114, 114))\r\n",
      "\r\n",
      "    # Visualize\r\n",
      "    # import matplotlib.pyplot as plt\r\n",
      "    # ax = plt.subplots(1, 2, figsize=(12, 6))[1].ravel()\r\n",
      "    # ax[0].imshow(im[:, :, ::-1])  # base\r\n",
      "    # ax[1].imshow(im2[:, :, ::-1])  # warped\r\n",
      "\r\n",
      "    # Transform label coordinates\r\n",
      "    n = len(targets)\r\n",
      "    if n:\r\n",
      "        use_segments = any(x.any() for x in segments)\r\n",
      "        new = np.zeros((n, 4))\r\n",
      "        if use_segments:  # warp segments\r\n",
      "            segments = resample_segments(segments)  # upsample\r\n",
      "            for i, segment in enumerate(segments):\r\n",
      "                xy = np.ones((len(segment), 3))\r\n",
      "                xy[:, :2] = segment\r\n",
      "                xy = xy @ M.T  # transform\r\n",
      "                xy = xy[:, :2] / xy[:, 2:3] if perspective else xy[:, :2]  # perspective rescale or affine\r\n",
      "\r\n",
      "                # clip\r\n",
      "                new[i] = segment2box(xy, width, height)\r\n",
      "\r\n",
      "        else:  # warp boxes\r\n",
      "            xy = np.ones((n * 4, 3))\r\n",
      "            xy[:, :2] = targets[:, [1, 2, 3, 4, 1, 4, 3, 2]].reshape(n * 4, 2)  # x1y1, x2y2, x1y2, x2y1\r\n",
      "            xy = xy @ M.T  # transform\r\n",
      "            xy = (xy[:, :2] / xy[:, 2:3] if perspective else xy[:, :2]).reshape(n, 8)  # perspective rescale or affine\r\n",
      "\r\n",
      "            # create new boxes\r\n",
      "            x = xy[:, [0, 2, 4, 6]]\r\n",
      "            y = xy[:, [1, 3, 5, 7]]\r\n",
      "            new = np.concatenate((x.min(1), y.min(1), x.max(1), y.max(1))).reshape(4, n).T\r\n",
      "\r\n",
      "            # clip\r\n",
      "            new[:, [0, 2]] = new[:, [0, 2]].clip(0, width)\r\n",
      "            new[:, [1, 3]] = new[:, [1, 3]].clip(0, height)\r\n",
      "\r\n",
      "        # filter candidates\r\n",
      "        i = box_candidates(box1=targets[:, 1:5].T * s, box2=new.T, area_thr=0.01 if use_segments else 0.10)\r\n",
      "        targets = targets[i]\r\n",
      "        targets[:, 1:5] = new[i]\r\n",
      "\r\n",
      "    return im, targets\r\n",
      "\r\n",
      "\r\n",
      "def copy_paste(im, labels, segments, p=0.5):\r\n",
      "    # Implement Copy-Paste augmentation https://arxiv.org/abs/2012.07177, labels as nx5 np.array(cls, xyxy)\r\n",
      "    n = len(segments)\r\n",
      "    if p and n:\r\n",
      "        h, w, c = im.shape  # height, width, channels\r\n",
      "        im_new = np.zeros(im.shape, np.uint8)\r\n",
      "        for j in random.sample(range(n), k=round(p * n)):\r\n",
      "            l, s = labels[j], segments[j]\r\n",
      "            box = w - l[3], l[2], w - l[1], l[4]\r\n",
      "            ioa = bbox_ioa(box, labels[:, 1:5])  # intersection over area\r\n",
      "            if (ioa < 0.30).all():  # allow 30% obscuration of existing labels\r\n",
      "                labels = np.concatenate((labels, [[l[0], *box]]), 0)\r\n",
      "                segments.append(np.concatenate((w - s[:, 0:1], s[:, 1:2]), 1))\r\n",
      "                cv2.drawContours(im_new, [segments[j].astype(np.int32)], -1, (255, 255, 255), cv2.FILLED)\r\n",
      "\r\n",
      "        result = cv2.bitwise_and(src1=im, src2=im_new)\r\n",
      "        result = cv2.flip(result, 1)  # augment segments (flip left-right)\r\n",
      "        i = result > 0  # pixels to replace\r\n",
      "        # i[:, :] = result.max(2).reshape(h, w, 1)  # act over ch\r\n",
      "        im[i] = result[i]  # cv2.imwrite('debug.jpg', im)  # debug\r\n",
      "\r\n",
      "    return im, labels, segments\r\n",
      "\r\n",
      "\r\n",
      "def cutout(im, labels, p=0.5):\r\n",
      "    # Applies image cutout augmentation https://arxiv.org/abs/1708.04552\r\n",
      "    if random.random() < p:\r\n",
      "        h, w = im.shape[:2]\r\n",
      "        scales = [0.5] * 1 + [0.25] * 2 + [0.125] * 4 + [0.0625] * 8 + [0.03125] * 16  # image size fraction\r\n",
      "        for s in scales:\r\n",
      "            mask_h = random.randint(1, int(h * s))  # create random masks\r\n",
      "            mask_w = random.randint(1, int(w * s))\r\n",
      "\r\n",
      "            # box\r\n",
      "            xmin = max(0, random.randint(0, w) - mask_w // 2)\r\n",
      "            ymin = max(0, random.randint(0, h) - mask_h // 2)\r\n",
      "            xmax = min(w, xmin + mask_w)\r\n",
      "            ymax = min(h, ymin + mask_h)\r\n",
      "\r\n",
      "            # apply random color mask\r\n",
      "            im[ymin:ymax, xmin:xmax] = [random.randint(64, 191) for _ in range(3)]\r\n",
      "\r\n",
      "            # return unobscured labels\r\n",
      "            if len(labels) and s > 0.03:\r\n",
      "                box = np.array([xmin, ymin, xmax, ymax], dtype=np.float32)\r\n",
      "                ioa = bbox_ioa(box, labels[:, 1:5])  # intersection over area\r\n",
      "                labels = labels[ioa < 0.60]  # remove >60% obscured labels\r\n",
      "\r\n",
      "    return labels\r\n",
      "\r\n",
      "\r\n",
      "def mixup(im, labels, im2, labels2):\r\n",
      "    # Applies MixUp augmentation https://arxiv.org/pdf/1710.09412.pdf\r\n",
      "    r = np.random.beta(32.0, 32.0)  # mixup ratio, alpha=beta=32.0\r\n",
      "    im = (im * r + im2 * (1 - r)).astype(np.uint8)\r\n",
      "    labels = np.concatenate((labels, labels2), 0)\r\n",
      "    return im, labels\r\n",
      "\r\n",
      "\r\n",
      "def box_candidates(box1, box2, wh_thr=2, ar_thr=100, area_thr=0.1, eps=1e-16):  # box1(4,n), box2(4,n)\r\n",
      "    # Compute candidate boxes: box1 before augment, box2 after augment, wh_thr (pixels), aspect_ratio_thr, area_ratio\r\n",
      "    w1, h1 = box1[2] - box1[0], box1[3] - box1[1]\r\n",
      "    w2, h2 = box2[2] - box2[0], box2[3] - box2[1]\r\n",
      "    ar = np.maximum(w2 / (h2 + eps), h2 / (w2 + eps))  # aspect ratio\r\n",
      "    return (w2 > wh_thr) & (h2 > wh_thr) & (w2 * h2 / (w1 * h1 + eps) > area_thr) & (ar < ar_thr)  # candidates\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "augmentations_template = '''\n",
    "# YOLOv5 ðŸš€ by Ultralytics, GPL-3.0 license\n",
    "\"\"\"\n",
    "Image augmentation functions\n",
    "\"\"\"\n",
    "\n",
    "import math\n",
    "import random\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "from utils.general import LOGGER, check_version, colorstr, resample_segments, segment2box\n",
    "from utils.metrics import bbox_ioa\n",
    "\n",
    "\n",
    "class Albumentations:\n",
    "    # YOLOv5 Albumentations class (optional, only used if package is installed)\n",
    "    def __init__(self):\n",
    "        self.transform = None\n",
    "        try:\n",
    "            import albumentations as A\n",
    "            # check_version(A.__version__, '1.0.3', hard=True)  # version requirement\n",
    "\n",
    "            self.transform = A.Compose($albumentations\n",
    "                bbox_params=A.BboxParams(format='yolo', label_fields=['class_labels']))\n",
    "\n",
    "            LOGGER.info(colorstr('albumentations: ') + ', '.join(f'{x}' for x in self.transform.transforms if x.p))\n",
    "        except ImportError:  # package not installed, skip\n",
    "            pass\n",
    "        except Exception as e:\n",
    "            LOGGER.info(colorstr('albumentations: ') + f'{e}')\n",
    "\n",
    "    def __call__(self, im, labels, p=1.0):\n",
    "        if self.transform and random.random() < p:\n",
    "            new = self.transform(image=im, bboxes=labels[:, 1:], class_labels=labels[:, 0])  # transformed\n",
    "            im, labels = new['image'], np.array([[c, *b] for c, b in zip(new['class_labels'], new['bboxes'])])\n",
    "        return im, labels\n",
    "\n",
    "\n",
    "def augment_hsv(im, hgain=0.5, sgain=0.5, vgain=0.5):\n",
    "    # HSV color-space augmentation\n",
    "    if hgain or sgain or vgain:\n",
    "        r = np.random.uniform(-1, 1, 3) * [hgain, sgain, vgain] + 1  # random gains\n",
    "        hue, sat, val = cv2.split(cv2.cvtColor(im, cv2.COLOR_BGR2HSV))\n",
    "        dtype = im.dtype  # uint8\n",
    "\n",
    "        x = np.arange(0, 256, dtype=r.dtype)\n",
    "        lut_hue = ((x * r[0]) % 180).astype(dtype)\n",
    "        lut_sat = np.clip(x * r[1], 0, 255).astype(dtype)\n",
    "        lut_val = np.clip(x * r[2], 0, 255).astype(dtype)\n",
    "\n",
    "        im_hsv = cv2.merge((cv2.LUT(hue, lut_hue), cv2.LUT(sat, lut_sat), cv2.LUT(val, lut_val)))\n",
    "        cv2.cvtColor(im_hsv, cv2.COLOR_HSV2BGR, dst=im)  # no return needed\n",
    "\n",
    "\n",
    "def hist_equalize(im, clahe=True, bgr=False):\n",
    "    # Equalize histogram on BGR image 'im' with im.shape(n,m,3) and range 0-255\n",
    "    yuv = cv2.cvtColor(im, cv2.COLOR_BGR2YUV if bgr else cv2.COLOR_RGB2YUV)\n",
    "    if clahe:\n",
    "        c = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "        yuv[:, :, 0] = c.apply(yuv[:, :, 0])\n",
    "    else:\n",
    "        yuv[:, :, 0] = cv2.equalizeHist(yuv[:, :, 0])  # equalize Y channel histogram\n",
    "    return cv2.cvtColor(yuv, cv2.COLOR_YUV2BGR if bgr else cv2.COLOR_YUV2RGB)  # convert YUV image to RGB\n",
    "\n",
    "\n",
    "def replicate(im, labels):\n",
    "    # Replicate labels\n",
    "    h, w = im.shape[:2]\n",
    "    boxes = labels[:, 1:].astype(int)\n",
    "    x1, y1, x2, y2 = boxes.T\n",
    "    s = ((x2 - x1) + (y2 - y1)) / 2  # side length (pixels)\n",
    "    for i in s.argsort()[:round(s.size * 0.5)]:  # smallest indices\n",
    "        x1b, y1b, x2b, y2b = boxes[i]\n",
    "        bh, bw = y2b - y1b, x2b - x1b\n",
    "        yc, xc = int(random.uniform(0, h - bh)), int(random.uniform(0, w - bw))  # offset x, y\n",
    "        x1a, y1a, x2a, y2a = [xc, yc, xc + bw, yc + bh]\n",
    "        im[y1a:y2a, x1a:x2a] = im[y1b:y2b, x1b:x2b]  # im4[ymin:ymax, xmin:xmax]\n",
    "        labels = np.append(labels, [[labels[i, 0], x1a, y1a, x2a, y2a]], axis=0)\n",
    "\n",
    "    return im, labels\n",
    "\n",
    "\n",
    "def letterbox(im, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True, stride=32):\n",
    "    # Resize and pad image while meeting stride-multiple constraints\n",
    "    shape = im.shape[:2]  # current shape [height, width]\n",
    "    if isinstance(new_shape, int):\n",
    "        new_shape = (new_shape, new_shape)\n",
    "\n",
    "    # Scale ratio (new / old)\n",
    "    r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n",
    "    if not scaleup:  # only scale down, do not scale up (for better val mAP)\n",
    "        r = min(r, 1.0)\n",
    "\n",
    "    # Compute padding\n",
    "    ratio = r, r  # width, height ratios\n",
    "    new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\n",
    "    dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding\n",
    "    if auto:  # minimum rectangle\n",
    "        dw, dh = np.mod(dw, stride), np.mod(dh, stride)  # wh padding\n",
    "    elif scaleFill:  # stretch\n",
    "        dw, dh = 0.0, 0.0\n",
    "        new_unpad = (new_shape[1], new_shape[0])\n",
    "        ratio = new_shape[1] / shape[1], new_shape[0] / shape[0]  # width, height ratios\n",
    "\n",
    "    dw /= 2  # divide padding into 2 sides\n",
    "    dh /= 2\n",
    "\n",
    "    if shape[::-1] != new_unpad:  # resize\n",
    "        im = cv2.resize(im, new_unpad, interpolation=cv2.INTER_LINEAR)\n",
    "    top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))\n",
    "    left, right = int(round(dw - 0.1)), int(round(dw + 0.1))\n",
    "    im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)  # add border\n",
    "    return im, ratio, (dw, dh)\n",
    "\n",
    "\n",
    "def random_perspective(im, targets=(), segments=(), degrees=10, translate=.1, scale=.1, shear=10, perspective=0.0,\n",
    "                       border=(0, 0)):\n",
    "    # torchvision.transforms.RandomAffine(degrees=(-10, 10), translate=(0.1, 0.1), scale=(0.9, 1.1), shear=(-10, 10))\n",
    "    # targets = [cls, xyxy]\n",
    "\n",
    "    height = im.shape[0] + border[0] * 2  # shape(h,w,c)\n",
    "    width = im.shape[1] + border[1] * 2\n",
    "\n",
    "    # Center\n",
    "    C = np.eye(3)\n",
    "    C[0, 2] = -im.shape[1] / 2  # x translation (pixels)\n",
    "    C[1, 2] = -im.shape[0] / 2  # y translation (pixels)\n",
    "\n",
    "    # Perspective\n",
    "    P = np.eye(3)\n",
    "    P[2, 0] = random.uniform(-perspective, perspective)  # x perspective (about y)\n",
    "    P[2, 1] = random.uniform(-perspective, perspective)  # y perspective (about x)\n",
    "\n",
    "    # Rotation and Scale\n",
    "    R = np.eye(3)\n",
    "    a = random.uniform(-degrees, degrees)\n",
    "    # a += random.choice([-180, -90, 0, 90])  # add 90deg rotations to small rotations\n",
    "    s = random.uniform(1 - scale, 1 + scale)\n",
    "    # s = 2 ** random.uniform(-scale, scale)\n",
    "    R[:2] = cv2.getRotationMatrix2D(angle=a, center=(0, 0), scale=s)\n",
    "\n",
    "    # Shear\n",
    "    S = np.eye(3)\n",
    "    S[0, 1] = math.tan(random.uniform(-shear, shear) * math.pi / 180)  # x shear (deg)\n",
    "    S[1, 0] = math.tan(random.uniform(-shear, shear) * math.pi / 180)  # y shear (deg)\n",
    "\n",
    "    # Translation\n",
    "    T = np.eye(3)\n",
    "    T[0, 2] = random.uniform(0.5 - translate, 0.5 + translate) * width  # x translation (pixels)\n",
    "    T[1, 2] = random.uniform(0.5 - translate, 0.5 + translate) * height  # y translation (pixels)\n",
    "\n",
    "    # Combined rotation matrix\n",
    "    M = T @ S @ R @ P @ C  # order of operations (right to left) is IMPORTANT\n",
    "    if (border[0] != 0) or (border[1] != 0) or (M != np.eye(3)).any():  # image changed\n",
    "        if perspective:\n",
    "            im = cv2.warpPerspective(im, M, dsize=(width, height), borderValue=(114, 114, 114))\n",
    "        else:  # affine\n",
    "            im = cv2.warpAffine(im, M[:2], dsize=(width, height), borderValue=(114, 114, 114))\n",
    "\n",
    "    # Visualize\n",
    "    # import matplotlib.pyplot as plt\n",
    "    # ax = plt.subplots(1, 2, figsize=(12, 6))[1].ravel()\n",
    "    # ax[0].imshow(im[:, :, ::-1])  # base\n",
    "    # ax[1].imshow(im2[:, :, ::-1])  # warped\n",
    "\n",
    "    # Transform label coordinates\n",
    "    n = len(targets)\n",
    "    if n:\n",
    "        use_segments = any(x.any() for x in segments)\n",
    "        new = np.zeros((n, 4))\n",
    "        if use_segments:  # warp segments\n",
    "            segments = resample_segments(segments)  # upsample\n",
    "            for i, segment in enumerate(segments):\n",
    "                xy = np.ones((len(segment), 3))\n",
    "                xy[:, :2] = segment\n",
    "                xy = xy @ M.T  # transform\n",
    "                xy = xy[:, :2] / xy[:, 2:3] if perspective else xy[:, :2]  # perspective rescale or affine\n",
    "\n",
    "                # clip\n",
    "                new[i] = segment2box(xy, width, height)\n",
    "\n",
    "        else:  # warp boxes\n",
    "            xy = np.ones((n * 4, 3))\n",
    "            xy[:, :2] = targets[:, [1, 2, 3, 4, 1, 4, 3, 2]].reshape(n * 4, 2)  # x1y1, x2y2, x1y2, x2y1\n",
    "            xy = xy @ M.T  # transform\n",
    "            xy = (xy[:, :2] / xy[:, 2:3] if perspective else xy[:, :2]).reshape(n, 8)  # perspective rescale or affine\n",
    "\n",
    "            # create new boxes\n",
    "            x = xy[:, [0, 2, 4, 6]]\n",
    "            y = xy[:, [1, 3, 5, 7]]\n",
    "            new = np.concatenate((x.min(1), y.min(1), x.max(1), y.max(1))).reshape(4, n).T\n",
    "\n",
    "            # clip\n",
    "            new[:, [0, 2]] = new[:, [0, 2]].clip(0, width)\n",
    "            new[:, [1, 3]] = new[:, [1, 3]].clip(0, height)\n",
    "\n",
    "        # filter candidates\n",
    "        i = box_candidates(box1=targets[:, 1:5].T * s, box2=new.T, area_thr=0.01 if use_segments else 0.10)\n",
    "        targets = targets[i]\n",
    "        targets[:, 1:5] = new[i]\n",
    "\n",
    "    return im, targets\n",
    "\n",
    "\n",
    "def copy_paste(im, labels, segments, p=0.5):\n",
    "    # Implement Copy-Paste augmentation https://arxiv.org/abs/2012.07177, labels as nx5 np.array(cls, xyxy)\n",
    "    n = len(segments)\n",
    "    if p and n:\n",
    "        h, w, c = im.shape  # height, width, channels\n",
    "        im_new = np.zeros(im.shape, np.uint8)\n",
    "        for j in random.sample(range(n), k=round(p * n)):\n",
    "            l, s = labels[j], segments[j]\n",
    "            box = w - l[3], l[2], w - l[1], l[4]\n",
    "            ioa = bbox_ioa(box, labels[:, 1:5])  # intersection over area\n",
    "            if (ioa < 0.30).all():  # allow 30% obscuration of existing labels\n",
    "                labels = np.concatenate((labels, [[l[0], *box]]), 0)\n",
    "                segments.append(np.concatenate((w - s[:, 0:1], s[:, 1:2]), 1))\n",
    "                cv2.drawContours(im_new, [segments[j].astype(np.int32)], -1, (255, 255, 255), cv2.FILLED)\n",
    "\n",
    "        result = cv2.bitwise_and(src1=im, src2=im_new)\n",
    "        result = cv2.flip(result, 1)  # augment segments (flip left-right)\n",
    "        i = result > 0  # pixels to replace\n",
    "        # i[:, :] = result.max(2).reshape(h, w, 1)  # act over ch\n",
    "        im[i] = result[i]  # cv2.imwrite('debug.jpg', im)  # debug\n",
    "\n",
    "    return im, labels, segments\n",
    "\n",
    "\n",
    "def cutout(im, labels, p=0.5):\n",
    "    # Applies image cutout augmentation https://arxiv.org/abs/1708.04552\n",
    "    if random.random() < p:\n",
    "        h, w = im.shape[:2]\n",
    "        scales = [0.5] * 1 + [0.25] * 2 + [0.125] * 4 + [0.0625] * 8 + [0.03125] * 16  # image size fraction\n",
    "        for s in scales:\n",
    "            mask_h = random.randint(1, int(h * s))  # create random masks\n",
    "            mask_w = random.randint(1, int(w * s))\n",
    "\n",
    "            # box\n",
    "            xmin = max(0, random.randint(0, w) - mask_w // 2)\n",
    "            ymin = max(0, random.randint(0, h) - mask_h // 2)\n",
    "            xmax = min(w, xmin + mask_w)\n",
    "            ymax = min(h, ymin + mask_h)\n",
    "\n",
    "            # apply random color mask\n",
    "            im[ymin:ymax, xmin:xmax] = [random.randint(64, 191) for _ in range(3)]\n",
    "\n",
    "            # return unobscured labels\n",
    "            if len(labels) and s > 0.03:\n",
    "                box = np.array([xmin, ymin, xmax, ymax], dtype=np.float32)\n",
    "                ioa = bbox_ioa(box, labels[:, 1:5])  # intersection over area\n",
    "                labels = labels[ioa < 0.60]  # remove >60% obscured labels\n",
    "\n",
    "    return labels\n",
    "\n",
    "\n",
    "def mixup(im, labels, im2, labels2):\n",
    "    # Applies MixUp augmentation https://arxiv.org/pdf/1710.09412.pdf\n",
    "    r = np.random.beta(32.0, 32.0)  # mixup ratio, alpha=beta=32.0\n",
    "    im = (im * r + im2 * (1 - r)).astype(np.uint8)\n",
    "    labels = np.concatenate((labels, labels2), 0)\n",
    "    return im, labels\n",
    "\n",
    "\n",
    "def box_candidates(box1, box2, wh_thr=2, ar_thr=100, area_thr=0.1, eps=1e-16):  # box1(4,n), box2(4,n)\n",
    "    # Compute candidate boxes: box1 before augment, box2 after augment, wh_thr (pixels), aspect_ratio_thr, area_ratio\n",
    "    w1, h1 = box1[2] - box1[0], box1[3] - box1[1]\n",
    "    w2, h2 = box2[2] - box2[0], box2[3] - box2[1]\n",
    "    ar = np.maximum(w2 / (h2 + eps), h2 / (w2 + eps))  # aspect ratio\n",
    "    return (w2 > wh_thr) & (h2 > wh_thr) & (w2 * h2 / (w1 * h1 + eps) > area_thr) & (ar < ar_thr)  # candidates\n",
    "\n",
    "'''\n",
    "\n",
    "augmentations = Template(augmentations_template).substitute(\n",
    "    albumentations = ALBUMENTATIONS,\n",
    ")\n",
    "\n",
    "with open('/kaggle/working/yolov5/utils/augmentations.py', 'w') as f:\n",
    "    f.write(augmentations)\n",
    "\n",
    "!cat /kaggle/working/yolov5/utils/augmentations.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ca0cc89",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-11T23:13:59.295500Z",
     "iopub.status.busy": "2022-02-11T23:13:59.294973Z",
     "iopub.status.idle": "2022-02-11T23:13:59.765252Z",
     "shell.execute_reply": "2022-02-11T23:13:59.765715Z",
     "shell.execute_reply.started": "2022-02-11T22:44:12.388198Z"
    },
    "papermill": {
     "duration": 0.504337,
     "end_time": "2022-02-11T23:13:59.765881",
     "exception": false,
     "start_time": "2022-02-11T23:13:59.261544",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>sequence</th>\n",
       "      <th>video_frame</th>\n",
       "      <th>sequence_frame</th>\n",
       "      <th>image_id</th>\n",
       "      <th>annotations</th>\n",
       "      <th>annotations1</th>\n",
       "      <th>image_path</th>\n",
       "      <th>no_of_bbox</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>40258</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0-0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>video_0/0.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>40258</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0-1</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>video_0/1.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>40258</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0-2</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>video_0/2.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>40258</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0-3</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>video_0/3.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>40258</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0-4</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>video_0/4.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   video_id  sequence  video_frame  sequence_frame image_id annotations  \\\n",
       "0         0     40258            0               0      0-0          []   \n",
       "1         0     40258            1               1      0-1          []   \n",
       "2         0     40258            2               2      0-2          []   \n",
       "3         0     40258            3               3      0-3          []   \n",
       "4         0     40258            4               4      0-4          []   \n",
       "\n",
       "  annotations1     image_path  no_of_bbox  \n",
       "0           []  video_0/0.jpg           0  \n",
       "1           []  video_0/1.jpg           0  \n",
       "2           []  video_0/2.jpg           0  \n",
       "3           []  video_0/3.jpg           0  \n",
       "4           []  video_0/4.jpg           0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../input/tensorflow-great-barrier-reef/train.csv\")\n",
    "\n",
    "# Turn annotations from strings into lists of dictionaries\n",
    "df['annotations1'] = df['annotations'].apply(eval)\n",
    "\n",
    "# Create the image path for the row\n",
    "df['image_path'] = \"video_\" + df['video_id'].astype(str) + \"/\" + df['video_frame'].astype(str) + \".jpg\"\n",
    "\n",
    "length = lambda x: len(x) \n",
    "\n",
    "df[\"no_of_bbox\"] = df[\"annotations1\"].apply(length)\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec5fe62c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-11T23:13:59.827086Z",
     "iopub.status.busy": "2022-02-11T23:13:59.826035Z",
     "iopub.status.idle": "2022-02-11T23:13:59.837498Z",
     "shell.execute_reply": "2022-02-11T23:13:59.837911Z",
     "shell.execute_reply.started": "2022-02-11T22:44:12.874889Z"
    },
    "papermill": {
     "duration": 0.043249,
     "end_time": "2022-02-11T23:13:59.838049",
     "exception": false,
     "start_time": "2022-02-11T23:13:59.794800",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(677, 9)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df = df[df[\"video_id\"]==2][df[\"no_of_bbox\"]>0]\n",
    "val_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01ea8508",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-11T23:13:59.899350Z",
     "iopub.status.busy": "2022-02-11T23:13:59.898501Z",
     "iopub.status.idle": "2022-02-11T23:13:59.905352Z",
     "shell.execute_reply": "2022-02-11T23:13:59.905881Z",
     "shell.execute_reply.started": "2022-02-11T22:44:12.890920Z"
    },
    "papermill": {
     "duration": 0.039927,
     "end_time": "2022-02-11T23:13:59.906032",
     "exception": false,
     "start_time": "2022-02-11T23:13:59.866105",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4242, 9)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = df[df[\"video_id\"]!=2][df[\"no_of_bbox\"]>0]\n",
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14e07cac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-11T23:13:59.976169Z",
     "iopub.status.busy": "2022-02-11T23:13:59.968996Z",
     "iopub.status.idle": "2022-02-11T23:14:02.686662Z",
     "shell.execute_reply": "2022-02-11T23:14:02.685368Z",
     "shell.execute_reply.started": "2022-02-11T22:44:12.905021Z"
    },
    "papermill": {
     "duration": 2.752741,
     "end_time": "2022-02-11T23:14:02.686815",
     "exception": false,
     "start_time": "2022-02-11T23:13:59.934074",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir -p ./yolo_data/fold2/images/val\n",
    "!mkdir -p ./yolo_data/fold2/images/train\n",
    "\n",
    "!mkdir -p ./yolo_data/fold2/labels/val\n",
    "!mkdir -p ./yolo_data/fold2/labels/train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "adb82b67",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-11T23:14:02.753271Z",
     "iopub.status.busy": "2022-02-11T23:14:02.752751Z",
     "iopub.status.idle": "2022-02-11T23:14:11.266164Z",
     "shell.execute_reply": "2022-02-11T23:14:11.265661Z",
     "shell.execute_reply.started": "2022-02-11T22:44:15.674602Z"
    },
    "papermill": {
     "duration": 8.550715,
     "end_time": "2022-02-11T23:14:11.266295",
     "exception": false,
     "start_time": "2022-02-11T23:14:02.715580",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fold = 2\n",
    "\n",
    "annos = []\n",
    "for i, x in val_df.iterrows():\n",
    "#     if x.video_id == fold:\n",
    "#         if x.pos:\n",
    "    mode = 'val'\n",
    "#     else:\n",
    "#         # train\n",
    "#         mode = 'train'\n",
    "#         if not x.pos: continue\n",
    "        # val\n",
    "    copyfile(f'../input/tensorflow-great-barrier-reef/train_images/video_{x.video_id}/{x.video_frame}.jpg',\n",
    "                f'./yolo_data/fold{fold}/images/{mode}/{x.image_id}.jpg')\n",
    "#     if not x.pos:\n",
    "#         continue\n",
    "    r = ''\n",
    "    anno = eval(x.annotations)\n",
    "    for an in anno:\n",
    "#            annos.append(an)\n",
    "        r += '0 {} {} {} {}\\n'.format((an['x'] + an['width'] / 2) / 1280,\n",
    "                                        (an['y'] + an['height'] / 2) / 720,\n",
    "                                        an['width'] / 1280, an['height'] / 720)\n",
    "    with open(f'./yolo_data/fold{fold}/labels/{mode}/{x.image_id}.txt', 'w') as fp:\n",
    "        fp.write(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af75095c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-11T23:14:11.332485Z",
     "iopub.status.busy": "2022-02-11T23:14:11.331485Z",
     "iopub.status.idle": "2022-02-11T23:14:59.231037Z",
     "shell.execute_reply": "2022-02-11T23:14:59.231533Z",
     "shell.execute_reply.started": "2022-02-11T22:44:22.822245Z"
    },
    "papermill": {
     "duration": 47.935888,
     "end_time": "2022-02-11T23:14:59.231728",
     "exception": false,
     "start_time": "2022-02-11T23:14:11.295840",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fold = 2\n",
    "\n",
    "annos = []\n",
    "for i, x in train_df.iterrows():\n",
    "#     if x.video_id == fold:\n",
    "#         if x.pos:\n",
    "    mode = 'train'\n",
    "#     else:\n",
    "#         # train\n",
    "#         mode = 'train'\n",
    "#         if not x.pos: continue\n",
    "        # val\n",
    "    copyfile(f'../input/tensorflow-great-barrier-reef/train_images/video_{x.video_id}/{x.video_frame}.jpg',\n",
    "                f'./yolo_data/fold{fold}/images/{mode}/{x.image_id}.jpg')\n",
    "#     if not x.pos:\n",
    "#         continue\n",
    "    r = ''\n",
    "    anno = eval(x.annotations)\n",
    "    for an in anno:\n",
    "#            annos.append(an)\n",
    "        r += '0 {} {} {} {}\\n'.format((an['x'] + an['width'] / 2) / 1280,\n",
    "                                        (an['y'] + an['height'] / 2) / 720,\n",
    "                                        an['width'] / 1280, an['height'] / 720)\n",
    "    with open(f'./yolo_data/fold{fold}/labels/{mode}/{x.image_id}.txt', 'w') as fp:\n",
    "        fp.write(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898eb31f",
   "metadata": {
    "papermill": {
     "duration": 0.241828,
     "end_time": "2022-02-11T23:14:59.503461",
     "exception": false,
     "start_time": "2022-02-11T23:14:59.261633",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d607a2b0",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2022-02-11T23:14:59.606899Z",
     "iopub.status.busy": "2022-02-11T23:14:59.605915Z",
     "iopub.status.idle": "2022-02-11T23:14:59.608284Z",
     "shell.execute_reply": "2022-02-11T23:14:59.607639Z",
     "shell.execute_reply.started": "2022-02-11T22:45:07.445418Z"
    },
    "papermill": {
     "duration": 0.056279,
     "end_time": "2022-02-11T23:14:59.608446",
     "exception": false,
     "start_time": "2022-02-11T23:14:59.552167",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fold = 1\n",
    "\n",
    "# annos = []\n",
    "# for i, x in train.iterrows():\n",
    "#     if x.video_id == fold:\n",
    "#         if x.pos:\n",
    "#             mode = 'val'\n",
    "#     else:\n",
    "#         # train\n",
    "#         mode = 'train'\n",
    "#         if not x.pos: continue\n",
    "#         # val\n",
    "#     copyfile(f'../input/tensorflow-great-barrier-reef/train_images/video_{x.video_id}/{x.video_frame}.jpg',\n",
    "#                 f'./yolo_data/fold{fold}/images/{mode}/{x.image_id}.jpg')\n",
    "#     if not x.pos:\n",
    "#         continue\n",
    "#     r = ''\n",
    "#     anno = eval(x.annotations)\n",
    "#     for an in anno:\n",
    "# #            annos.append(an)\n",
    "#         r += '0 {} {} {} {}\\n'.format((an['x'] + an['width'] / 2) / 1280,\n",
    "#                                         (an['y'] + an['height'] / 2) / 720,\n",
    "#                                         an['width'] / 1280, an['height'] / 720)\n",
    "#     with open(f'./yolo_data/fold{fold}/labels/{mode}/{x.image_id}.txt', 'w') as fp:\n",
    "#         fp.write(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d4accaeb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-11T23:14:59.710390Z",
     "iopub.status.busy": "2022-02-11T23:14:59.708871Z",
     "iopub.status.idle": "2022-02-11T23:14:59.715085Z",
     "shell.execute_reply": "2022-02-11T23:14:59.715788Z",
     "shell.execute_reply.started": "2022-02-11T22:45:07.456092Z"
    },
    "papermill": {
     "duration": 0.061437,
     "end_time": "2022-02-11T23:14:59.716000",
     "exception": false,
     "start_time": "2022-02-11T23:14:59.654563",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4242"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "len(os.listdir(\"./yolo_data/fold2/labels/train\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "60c7f648",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-11T23:14:59.815476Z",
     "iopub.status.busy": "2022-02-11T23:14:59.814663Z",
     "iopub.status.idle": "2022-02-11T23:14:59.817557Z",
     "shell.execute_reply": "2022-02-11T23:14:59.818224Z",
     "shell.execute_reply.started": "2022-02-11T22:45:07.469238Z"
    },
    "papermill": {
     "duration": 0.056055,
     "end_time": "2022-02-11T23:14:59.818386",
     "exception": false,
     "start_time": "2022-02-11T23:14:59.762331",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "hyps = '''\n",
    "# YOLOv5 by Ultralytics, GPL-3.0 license\n",
    "# Hyperparameters for COCO training from scratch\n",
    "# python train.py --batch 40 --cfg yolov5m.yaml --weights '' --data coco.yaml --img 640 --epochs 300\n",
    "# See tutorials for hyperparameter evolution https://github.com/ultralytics/yolov5#tutorials\n",
    "\n",
    "lr0: 0.01  # initial learning rate (SGD=1E-2, Adam=1E-3)\n",
    "lrf: 0.1  # final OneCycleLR learning rate (lr0 * lrf)\n",
    "momentum: 0.937  # SGD momentum/Adam beta1\n",
    "weight_decay: 0.0005  # optimizer weight decay 5e-4\n",
    "warmup_epochs: 3.0  # warmup epochs (fractions ok)\n",
    "warmup_momentum: 0.8  # warmup initial momentum\n",
    "warmup_bias_lr: 0.1  # warmup initial bias lr\n",
    "box: 0.05  # box loss gain\n",
    "cls: 0.5  # cls loss gain\n",
    "cls_pw: 1.0  # cls BCELoss positive_weight\n",
    "obj: 1.0  # obj loss gain (scale with pixels)\n",
    "obj_pw: 1.0  # obj BCELoss positive_weight\n",
    "iou_t: 0.20  # IoU training threshold\n",
    "anchor_t: 4.0  # anchor-multiple threshold\n",
    "# anchors: 3  # anchors per output layer (0 to ignore)\n",
    "fl_gamma: 0.0  # focal loss gamma (efficientDet default gamma=1.5)\n",
    "hsv_h: 0.015  # image HSV-Hue augmentation (fraction)\n",
    "hsv_s: 0.7  # image HSV-Saturation augmentation (fraction)\n",
    "hsv_v: 0.4  # image HSV-Value augmentation (fraction)\n",
    "degrees: 0.0  # image rotation (+/- deg)\n",
    "translate: 0.1  # image translation (+/- fraction)\n",
    "scale: 0.5  # image scale (+/- gain)\n",
    "shear: 0.0  # image shear (+/- deg)\n",
    "perspective: 0.0  # image perspective (+/- fraction), range 0-0.001\n",
    "flipud: 0.5  # image flip up-down (probability)\n",
    "fliplr: 0.5  # image flip left-right (probability)\n",
    "mosaic: 1.0  # image mosaic (probability)\n",
    "mixup: 0.5  # image mixup (probability)\n",
    "copy_paste: 0.0  # segment copy-paste (probability)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2026fbb6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-11T23:14:59.916099Z",
     "iopub.status.busy": "2022-02-11T23:14:59.915107Z",
     "iopub.status.idle": "2022-02-11T23:14:59.917174Z",
     "shell.execute_reply": "2022-02-11T23:14:59.917952Z",
     "shell.execute_reply.started": "2022-02-11T22:45:07.476377Z"
    },
    "papermill": {
     "duration": 0.053374,
     "end_time": "2022-02-11T23:14:59.918132",
     "exception": false,
     "start_time": "2022-02-11T23:14:59.864758",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = '''\n",
    "# Train/val/test sets as 1) dir: path/to/imgs, 2) file: path/to/imgs.txt, or 3) list: [path/to/imgs1, path/to/imgs2, ..]\n",
    "\n",
    "path: ../yolo_data/fold2/  # dataset root dir\n",
    "train: images/train  # train images (relative to 'path') 128 images\n",
    "val: images/val  # val images (relative to 'path') 128 images\n",
    "test:  # test images (optional)\n",
    "\n",
    "# Classes\n",
    "nc: 1  # number of classes\n",
    "names: ['reef']  # class names\n",
    "\n",
    "\n",
    "# Download script/URL (optional)\n",
    "# download: https://ultralytics.com/assets/coco128.zip\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b6a8934e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-11T23:14:59.984453Z",
     "iopub.status.busy": "2022-02-11T23:14:59.983832Z",
     "iopub.status.idle": "2022-02-11T23:14:59.986887Z",
     "shell.execute_reply": "2022-02-11T23:14:59.986411Z",
     "shell.execute_reply.started": "2022-02-11T22:45:07.487076Z"
    },
    "papermill": {
     "duration": 0.035836,
     "end_time": "2022-02-11T23:14:59.987001",
     "exception": false,
     "start_time": "2022-02-11T23:14:59.951165",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('./yolov5/data/reef_f1_naive.yaml', 'w') as fp:\n",
    "    fp.write(data)\n",
    "with open('./yolov5/data/hyps/hyp.heavy.2.yaml', 'w') as fp:\n",
    "    fp.write(hyps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "daabb0af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-11T23:15:00.049650Z",
     "iopub.status.busy": "2022-02-11T23:15:00.049091Z",
     "iopub.status.idle": "2022-02-11T23:15:00.052561Z",
     "shell.execute_reply": "2022-02-11T23:15:00.053012Z",
     "shell.execute_reply.started": "2022-02-11T22:45:07.497249Z"
    },
    "papermill": {
     "duration": 0.037597,
     "end_time": "2022-02-11T23:15:00.053130",
     "exception": false,
     "start_time": "2022-02-11T23:15:00.015533",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/yolov5\n"
     ]
    }
   ],
   "source": [
    "%cd yolov5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "497693a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-11T23:15:00.114977Z",
     "iopub.status.busy": "2022-02-11T23:15:00.114269Z",
     "iopub.status.idle": "2022-02-11T23:15:00.792422Z",
     "shell.execute_reply": "2022-02-11T23:15:00.791895Z",
     "shell.execute_reply.started": "2022-02-11T22:45:07.508899Z"
    },
    "papermill": {
     "duration": 0.710672,
     "end_time": "2022-02-11T23:15:00.792541",
     "exception": false,
     "start_time": "2022-02-11T23:15:00.081869",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Argoverse.yaml\t      VOC.yaml\t     hyps\t\t xView.yaml\r\n",
      "GlobalWheat2020.yaml  VisDrone.yaml  images\r\n",
      "Objects365.yaml       coco.yaml      reef_f1_naive.yaml\r\n",
      "SKU-110K.yaml\t      coco128.yaml   scripts\r\n"
     ]
    }
   ],
   "source": [
    "!ls data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "841cb0c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-11T23:15:00.858695Z",
     "iopub.status.busy": "2022-02-11T23:15:00.857978Z",
     "iopub.status.idle": "2022-02-11T23:28:10.190847Z",
     "shell.execute_reply": "2022-02-11T23:28:10.191984Z",
     "shell.execute_reply.started": "2022-02-11T22:45:08.183071Z"
    },
    "papermill": {
     "duration": 789.369826,
     "end_time": "2022-02-11T23:28:10.192185",
     "exception": false,
     "start_time": "2022-02-11T23:15:00.822359",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W&B disabled.\r\n",
      "Downloading https://github.com/ultralytics/yolov5/releases/download/v6.0/yolov5m6.pt to yolov5m6.pt...\r\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68.7M/68.7M [00:04<00:00, 15.1MB/s]\r\n",
      "\r\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning '../yolo_data/fold2/labels/train' images and labels...4242 found\u001b[0m\r\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning '../yolo_data/fold2/labels/val' images and labels...677 found, 0 m\u001b[0m\r\n",
      "       0/0     1.33G   0.04832    0.0208         0        14       640: 100%|â–ˆâ–ˆâ–ˆ\r\n",
      "               Class     Images     Labels          P          R         F2     \r\n",
      "               Class     Images     Labels          P          R         F2     \r\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fa4a69410e0>\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1262, in _shutdown_workers\r\n",
      "AttributeError: 'NoneType' object has no attribute 'python_exit_status'\r\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fa4a69410e0>\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1262, in _shutdown_workers\r\n",
      "AttributeError: 'NoneType' object has no attribute 'python_exit_status'\r\n"
     ]
    }
   ],
   "source": [
    "!python -m wandb disabled\n",
    "\n",
    "!python train.py \\\n",
    "    --img 640 \\\n",
    "    --batch 2 \\\n",
    "    --epochs 1 \\\n",
    "    --data data/reef_f1_naive.yaml \\\n",
    "    --weights yolov5m6.pt \\\n",
    "    --name base_vid_2val \\\n",
    "    --hyp data/hyps/hyp.heavy.2.yaml \\\n",
    "    --save-period 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3ff9385f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-11T23:28:12.112785Z",
     "iopub.status.busy": "2022-02-11T23:28:12.111838Z",
     "iopub.status.idle": "2022-02-11T23:28:12.113742Z",
     "shell.execute_reply": "2022-02-11T23:28:12.114160Z",
     "shell.execute_reply.started": "2022-02-11T22:50:09.652996Z"
    },
    "papermill": {
     "duration": 1.097762,
     "end_time": "2022-02-11T23:28:12.114296",
     "exception": false,
     "start_time": "2022-02-11T23:28:11.016534",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !python -m wandb disabled\n",
    "\n",
    "# !python train.py \\\n",
    "#     --img 3000 \\\n",
    "#     --batch 2 \\\n",
    "#     --epochs 11 \\\n",
    "#     --data data/reef_f1_naive.yaml \\\n",
    "#     --weights yolov5s6.pt \\\n",
    "#     --name cots_with_albs \\\n",
    "#     --hyp data/hyps/hyp.heavy.2.yaml \\\n",
    "#     --save-period 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ae604e01",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-11T23:28:13.778654Z",
     "iopub.status.busy": "2022-02-11T23:28:13.777859Z",
     "iopub.status.idle": "2022-02-11T23:28:14.471103Z",
     "shell.execute_reply": "2022-02-11T23:28:14.469938Z",
     "shell.execute_reply.started": "2022-02-11T22:50:09.658947Z"
    },
    "papermill": {
     "duration": 1.51421,
     "end_time": "2022-02-11T23:28:14.471258",
     "exception": false,
     "start_time": "2022-02-11T23:28:12.957048",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTRIBUTING.md  __pycache__  hubconf.py\tsetup.cfg\tval.py\r\n",
      "Dockerfile\t data\t      models\t\ttrain.py\twandb\r\n",
      "LICENSE\t\t detect.py    requirements.txt\ttutorial.ipynb\tyolov5m6.pt\r\n",
      "README.md\t export.py    runs\t\tutils\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b39caca6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-11T23:28:16.124082Z",
     "iopub.status.busy": "2022-02-11T23:28:16.123243Z",
     "iopub.status.idle": "2022-02-11T23:28:20.538243Z",
     "shell.execute_reply": "2022-02-11T23:28:20.537804Z",
     "shell.execute_reply.started": "2022-02-11T22:50:10.348998Z"
    },
    "papermill": {
     "duration": 5.239645,
     "end_time": "2022-02-11T23:28:20.538364",
     "exception": false,
     "start_time": "2022-02-11T23:28:15.298719",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working\n",
      "rm: cannot remove '/kaggle/working/images': No such file or directory\r\n",
      "rm: cannot remove '/kaggle/working/labels': No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "%cd /kaggle/working\n",
    "\n",
    "!cp -r /kaggle/working/yolov5/runs/train/base_vid_2val /kaggle/working\n",
    "\n",
    "!cp /kaggle/working/yolov5/data/reef_f1_naive.yaml /kaggle/working/base_vid_2val/\n",
    "!cp /kaggle/working/yolov5/data/hyps/hyp.heavy.2.yaml /kaggle/working/base_vid_2val/\n",
    "# !cp /kaggle/working/yolov5/utils/augmentations.py /kaggle/working/base_vid_2val/\n",
    "\n",
    "!rm -r /kaggle/working/yolov5\n",
    "!rm -r /kaggle/working/images\n",
    "!rm -r /kaggle/working/labels"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 878.502629,
   "end_time": "2022-02-11T23:28:22.068804",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-02-11T23:13:43.566175",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
