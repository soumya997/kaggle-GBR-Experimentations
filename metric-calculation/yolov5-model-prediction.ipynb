{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\nimport numpy as np\nfrom tqdm.notebook import tqdm\ntqdm.pandas()\nimport pandas as pd\nimport os\nimport cv2\nimport matplotlib.pyplot as plt\nimport glob\nimport shutil\nimport sys\nsys.path.append('../input/tensorflow-great-barrier-reef')\nimport torch\nfrom PIL import Image\nimport ast\n\n\n\n# IMG_SIZE=2400\n# TRAIN_PATH = '/kaggle/input/tensorflow-great-barrier-reef'\n# #Best_Model = '/kaggle/input/barrie-reef-yolo5/yolov5/kaggle-Reef/exp/weights/best.pt'\n# Best_Model = '../input/tfreef-yolov5s6trainv1-weights/best.pt'","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-13T22:29:10.415334Z","iopub.execute_input":"2022-02-13T22:29:10.4156Z","iopub.status.idle":"2022-02-13T22:29:10.424165Z","shell.execute_reply.started":"2022-02-13T22:29:10.415572Z","shell.execute_reply":"2022-02-13T22:29:10.423452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !pip install -qU wandb\n!pip install -qU bbox-utility ","metadata":{"execution":{"iopub.status.busy":"2022-02-13T22:29:10.445565Z","iopub.execute_input":"2022-02-13T22:29:10.44587Z","iopub.status.idle":"2022-02-13T22:29:18.703315Z","shell.execute_reply.started":"2022-02-13T22:29:10.445843Z","shell.execute_reply":"2022-02-13T22:29:18.702483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ROOT_DIR  = '/kaggle/input/tensorflow-great-barrier-reef/'\n# CKPT_DIR  = '/kaggle/input/greatbarrierreef-yolov5-train-ds'\nCKPT_PATH = '../input/yolov5-w-albumentationstrain1/cots_with_albs/weights/epoch6.pt' # by @steamedsheep\nIMG_SIZE  = 10000#int(2000*3)  # \nCONF      = 0.275     # 1920*3 + conf-0.3\nIOU       = 0.2\nAUGMENT   = True","metadata":{"execution":{"iopub.status.busy":"2022-02-13T22:29:18.705747Z","iopub.execute_input":"2022-02-13T22:29:18.706044Z","iopub.status.idle":"2022-02-13T22:29:18.710914Z","shell.execute_reply.started":"2022-02-13T22:29:18.706002Z","shell.execute_reply":"2022-02-13T22:29:18.710051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train Data\ndf = pd.read_csv(f'{ROOT_DIR}/train.csv')\ndf['image_path'] = f'{ROOT_DIR}/train_images/video_'+df.video_id.astype(str)+'/'+df.video_frame.astype(str)+'.jpg'\ndf['annotations'] = df['annotations'].progress_apply(eval)\ndisplay(df.head(2))\n\ndf['num_bbox'] = df['annotations'].progress_apply(lambda x: len(x))\ndata = (df.num_bbox>0).value_counts()/len(df)*100\nprint(f\"No BBox: {data[0]:0.2f}% | With BBox: {data[1]:0.2f}%\")","metadata":{"execution":{"iopub.status.busy":"2022-02-13T22:29:18.71256Z","iopub.execute_input":"2022-02-13T22:29:18.712874Z","iopub.status.idle":"2022-02-13T22:29:19.208867Z","shell.execute_reply.started":"2022-02-13T22:29:18.71284Z","shell.execute_reply":"2022-02-13T22:29:19.208118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check https://github.com/awsaf49/bbox for source code of following utility functions\nfrom bbox.utils import coco2yolo, coco2voc, voc2yolo, voc2coco\nfrom bbox.utils import draw_bboxes, load_image\nfrom bbox.utils import clip_bbox, str2annot, annot2str\n\ndef get_bbox(annots):\n    bboxes = [list(annot.values()) for annot in annots]\n    return bboxes\n\ndef get_imgsize(row):\n    row['width'], row['height'] = imagesize.get(row['image_path'])\n    return row\n\nnp.random.seed(32)\ncolors = [(np.random.randint(255), np.random.randint(255), np.random.randint(255))\\\n          for idx in range(1)]","metadata":{"execution":{"iopub.status.busy":"2022-02-13T22:29:19.210147Z","iopub.execute_input":"2022-02-13T22:29:19.210478Z","iopub.status.idle":"2022-02-13T22:29:19.218329Z","shell.execute_reply.started":"2022-02-13T22:29:19.21044Z","shell.execute_reply":"2022-02-13T22:29:19.217297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir -p /root/.config/Ultralytics\n!cp ../input/yolo-arial/Arial.ttf /root/.config/Ultralytics/","metadata":{"execution":{"iopub.status.busy":"2022-02-13T22:29:19.220737Z","iopub.execute_input":"2022-02-13T22:29:19.221296Z","iopub.status.idle":"2022-02-13T22:29:20.663489Z","shell.execute_reply.started":"2022-02-13T22:29:19.221259Z","shell.execute_reply":"2022-02-13T22:29:20.662529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_model(ckpt_path, conf=0.25, iou=0.50):\n    model = torch.hub.load('/kaggle/input/yolov5-lib-ds',\n                           'custom',\n                           path=ckpt_path,\n                           source='local',\n                           force_reload=True)  # local repo\n    model.conf = conf  # NMS confidence threshold\n    model.iou  = iou  # NMS IoU threshold\n    model.classes = None   # (optional list) filter by class, i.e. = [0, 15, 16] for persons, cats and dogs\n    model.multi_label = False  # NMS multiple labels per box\n    model.max_det = 1000  # maximum number of detections per image\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-02-13T22:29:20.66696Z","iopub.execute_input":"2022-02-13T22:29:20.667191Z","iopub.status.idle":"2022-02-13T22:29:20.673076Z","shell.execute_reply.started":"2022-02-13T22:29:20.667161Z","shell.execute_reply":"2022-02-13T22:29:20.67231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = load_model(CKPT_PATH, conf=CONF, iou=IOU)\nimg = [\"../input/tensorflow-great-barrier-reef/train_images/video_0/1001.jpg\"]\nresults = model(img, size=1920, augment=True)\nresults.pandas().xyxy[0][\"class\"].values","metadata":{"execution":{"iopub.status.busy":"2022-02-13T22:29:20.674546Z","iopub.execute_input":"2022-02-13T22:29:20.675042Z","iopub.status.idle":"2022-02-13T22:29:21.107019Z","shell.execute_reply.started":"2022-02-13T22:29:20.675007Z","shell.execute_reply":"2022-02-13T22:29:21.106235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def np2list(np):\n    return [i.tolist() for i in np]\n\ndef predict(model, img, size=1920, augment=False):\n    height, width = img.shape[:2]\n    results = model(img, size=size, augment=augment)  # custom inference size\n    preds   = results.pandas().xyxy[0]\n    bboxes  = preds[['xmin','ymin','xmax','ymax']].values\n        \n    if len(bboxes):\n        bboxes  = voc2coco(bboxes,height,width).astype(int)\n        confs   = preds.confidence.values\n        clas   = preds[\"class\"].values\n\n        return np2list(bboxes), np2list(confs), np2list(clas)\n    else:\n        return [],[],[]\n    \n# def format_prediction(bboxes, confs):\n#     annot = ''\n#     if len(bboxes)>0:\n#         for idx in range(len(bboxes)):\n#             xmin, ymin, w, h = bboxes[idx]\n#             conf             = confs[idx]\n#             annot += f'{conf} {xmin} {ymin} {w} {h}'\n#             annot +=' '\n#         annot = annot.strip(' ')\n#     return annot\n\ndef show_img(img, bboxes, bbox_format='yolo'):\n    names  = ['starfish']*len(bboxes)\n    labels = [0]*len(bboxes)\n    img    = draw_bboxes(img = img,\n                           bboxes = bboxes, \n                           classes = names,\n                           class_ids = labels,\n                           class_name = True, \n                           colors = colors, \n                           bbox_format = bbox_format,\n                           line_thickness = 20)\n    return Image.fromarray(img).resize((800, 400))","metadata":{"execution":{"iopub.status.busy":"2022-02-13T22:29:21.108697Z","iopub.execute_input":"2022-02-13T22:29:21.109114Z","iopub.status.idle":"2022-02-13T22:29:21.120448Z","shell.execute_reply.started":"2022-02-13T22:29:21.109072Z","shell.execute_reply":"2022-02-13T22:29:21.119689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# %cd test\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nimport os\nimport time\nimport shutil\nimport torch.nn as nn\nfrom skimage import io\nimport torchvision\nimport cv2\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.models.detection import FasterRCNN\nfrom torchvision.models.detection.rpn import AnchorGenerator\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.utils.data.sampler import SequentialSampler\nfrom albumentations.pytorch import ToTensorV2\nfrom torchvision import utils\nimport albumentations as A\nfrom albumentations import (HorizontalFlip, ShiftScaleRotate, VerticalFlip, Normalize,Flip,\n                            Compose, GaussNoise)\nfrom shutil import copyfile\nfrom torchvision import transforms\nDEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\ndevice = DEVICE\nBASE_DIR = \"../input/tensorflow-great-barrier-reef/train_images/\"\n\n# num_epochs = 12\n# width = 852\n# height = 480","metadata":{"execution":{"iopub.status.busy":"2022-02-13T22:29:21.121991Z","iopub.execute_input":"2022-02-13T22:29:21.12226Z","iopub.status.idle":"2022-02-13T22:29:21.132188Z","shell.execute_reply.started":"2022-02-13T22:29:21.122218Z","shell.execute_reply":"2022-02-13T22:29:21.131361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"../input/tensorflow-great-barrier-reef/train.csv\")\n\n# Turn annotations from strings into lists of dictionaries\ndf['annotations1'] = df['annotations'].apply(eval)\n\n# Create the image path for the row\ndf['image_path'] = \"video_\" + df['video_id'].astype(str) + \"/\" + df['video_frame'].astype(str) + \".jpg\"\n\nlength = lambda x: len(x) \n\ndf[\"no_of_bbox\"] = df[\"annotations1\"].apply(length)\ndf['annotations'] = df['annotations'].apply(eval)\n\ndf.head(5)","metadata":{"execution":{"iopub.status.busy":"2022-02-13T22:29:21.134174Z","iopub.execute_input":"2022-02-13T22:29:21.134989Z","iopub.status.idle":"2022-02-13T22:29:21.69153Z","shell.execute_reply.started":"2022-02-13T22:29:21.134953Z","shell.execute_reply":"2022-02-13T22:29:21.690697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_df = df[df[\"video_id\"]==2][df[\"no_of_bbox\"]>0]\nval_df.shape","metadata":{"execution":{"iopub.status.busy":"2022-02-13T22:29:21.692995Z","iopub.execute_input":"2022-02-13T22:29:21.693273Z","iopub.status.idle":"2022-02-13T22:29:21.707617Z","shell.execute_reply.started":"2022-02-13T22:29:21.693225Z","shell.execute_reply":"2022-02-13T22:29:21.70687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ReefDataset:\n    \n    def __init__(self, df, transforms=None):\n        self.df = df\n        self.transforms = transforms\n        \n    def can_augment(self, boxes):\n        \"\"\" Check if bounding boxes are OK to augment\n        \n        \n        For example: image_id 1-490 has a bounding box that is partially outside of the image\n        It breaks albumentation\n        Here we check the margins are within the image to make sure the augmentation can be applied\n        \"\"\"\n        \n        box_outside_image = ((boxes[:, 0] < 0).any() or (boxes[:, 1] < 0).any() \n                             or (boxes[:, 2] > 1280).any() or (boxes[:, 3] > 720).any())\n        return not box_outside_image\n        \n    def get_boxes(self, row):\n        \"\"\"Returns the bboxes for a given row as a 3D matrix with format [x_min, y_min, x_max, y_max]\"\"\"\n        \n        boxes = pd.DataFrame(row['annotations'], columns=['x', 'y', 'width', 'height']).astype(float).values\n        \n        # Change from [x_min, y_min, w, h] to [x_min, y_min, x_max, y_max]\n        boxes[:, 2] = np.clip(boxes[:, 0] + boxes[:, 2],0,1280)\n        boxes[:, 3] = np.clip(boxes[:, 1] + boxes[:, 3],0,720)\n        return boxes\n    \n    def get_image(self, row):\n        \"\"\"Gets the image for a given row\"\"\"\n#         print(f'{BASE_DIR}{row[\"image_path\"]}')\n        image = cv2.imread(f'{BASE_DIR}{row[\"image_path\"]}', cv2.IMREAD_COLOR)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n#         image /= 255.0\n        return image,f'{BASE_DIR}{row[\"image_path\"]}'\n    \n    def __getitem__(self, i):\n\n        row = self.df.iloc[i]\n        image,pth = self.get_image(row)\n        boxes = self.get_boxes(row)\n        boxes1 = boxes\n        n_boxes = boxes.shape[0]\n        \n        # Calculate the area\n        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n        \n        \n        target = {\n            'boxes': torch.as_tensor(boxes, dtype=torch.float32),\n            'area': torch.as_tensor(area, dtype=torch.float32),\n            \n            'image_id': torch.tensor([i]),\n            \n            # There is only one class\n            'labels': torch.ones((n_boxes,), dtype=torch.int64), #unchanged\n            \n            # Suppose all instances are not crowd\n            'iscrowd': torch.zeros((n_boxes,), dtype=torch.int64) #unchanged   \n        }\n        \n#         if self.transforms is not None:\n#             image, target = self.transforms(image, target)\n#         return image, target\n\n        if self.transforms and self.can_augment(boxes):\n            sample = {\n                'image': image,\n                'bboxes': target['boxes'],\n                'labels': target['labels']\n            }\n            sample = self.transforms(**sample)\n            image = sample['image']\n#             target['boxes'] = torch.as_tensor(sample['bboxes'])\n#             if len(tensor.size()) != 0:\n#                 target['boxes'] = target['boxes'].int()\n#                 target['boxes'] = target['boxes'].to(torch.float32)\n#                 print(f\"{torch.tensor([i])} : {target['boxes'].shape}\")\n#                 print(f\"{torch.tensor([i])} : {target['boxes']}\")\n#                 print(\"#\"*80)\n#                 return image, target\n#             else:\n#                 continue\n                \n#             print(*sample['bboxes'][0])\n            if n_boxes > 0:\n#                 print(torch.tensor(sample['bboxes']))\n                target['boxes'] = torch.as_tensor(sample['bboxes']).int()\n                target['boxes'] = target['boxes'].to(torch.float32)\n#                 print(f\"{torch.tensor([i])} : {target['boxes'].shape}\")\n#                 print(f\"{torch.tensor([i])} : {target['boxes']}\")\n#                 print(\"#\"*80)\n\n        else:\n            image = ToTensorV2(p=1.0)(image=image)['image']\n            \n#         if target['boxes'].shape != (0):\n        return image, target, pth, boxes1\n\n    def __len__(self):\n        return len(self.df)","metadata":{"execution":{"iopub.status.busy":"2022-02-13T22:29:21.709232Z","iopub.execute_input":"2022-02-13T22:29:21.709529Z","iopub.status.idle":"2022-02-13T22:29:21.731531Z","shell.execute_reply.started":"2022-02-13T22:29:21.70949Z","shell.execute_reply":"2022-02-13T22:29:21.730768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_train_transform():\n    return A.Compose([\n#         A.Resize(height=height, width=width, p=1.0),\n#         A.Flip(0.0),\n        A.HorizontalFlip(0.8), # Same with transforms.RandomHorizontalFlip()\n\n#         A.VerticalFlip(0.1), \n\n#         A.Cutout(num_holes=25, max_h_size=6, max_w_size=6, fill_value=0, p=0.5),\n\n        ToTensorV2(p=1.0)\n    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n\n\ndef get_valid_transform():\n    return A.Compose([\n        ToTensorV2(p=1.0)\n    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n\nds_train = ReefDataset(val_df,get_train_transform())","metadata":{"execution":{"iopub.status.busy":"2022-02-13T22:29:21.733549Z","iopub.execute_input":"2022-02-13T22:29:21.734187Z","iopub.status.idle":"2022-02-13T22:29:21.742642Z","shell.execute_reply.started":"2022-02-13T22:29:21.734144Z","shell.execute_reply":"2022-02-13T22:29:21.741735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from PIL import Image \nfrom tqdm import tqdm\n\nbox_w_conf = []\ngt_bbox = []\n\nfor i in tqdm(range(len(val_df))):\n    \n    image, targets, pths, _ = ds_train[i]\n    boxes = targets['boxes'].cpu().numpy().astype(np.int32)\n\n    img1 = image.permute(1,2,0).cpu().numpy().astype(np.uint8)\n#     print(img1.shape)\n#     break\n#     from_arr = Image.fromarray(img1)\n    \n    bboxes, confis, clas = predict(model, img1, size=IMG_SIZE, augment=AUGMENT)\n    if bboxes == []:\n        demo = []\n    else:\n        demo = np.zeros((len(bboxes), 5))\n        demo[:,0] = confis\n        demo[:,1:5] = bboxes\n    box_w_conf.append(demo)\n    gt_bbox.append(boxes)","metadata":{"execution":{"iopub.status.busy":"2022-02-13T22:29:46.094646Z","iopub.execute_input":"2022-02-13T22:29:46.094932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model = load_model(CKPT_PATH, conf=CONF, iou=IOU)\n# image_paths = df[df.num_bbox>1].sample(100).image_path.tolist()\n# bboxes_list = []\n# config_list = []\n\n# box_w_conf = []\n# for idx, path in enumerate(image_paths):\n#     img = cv2.imread(path)[...,::-1]\n# #     img = cv2.resize(img,(10000,10000),cv2.INTER_LINEAR)\n    \n# #     print(img.shape)\n#     print(img.shape)\n#     bboxes, confis, clas = predict(model, img, size=IMG_SIZE, augment=AUGMENT)\n#     demo = np.zeros((len(bboxes), 5))\n#     demo[:,0] = confis\n#     demo[:,1:5] = bboxes\n#     box_w_conf.append(demo)\n# # #     break\n#     bboxes_list.append(bboxes)\n#     config_list.append(confis)\n#     display(show_img(img, bboxes, bbox_format='coco'))\n#     if idx>5:\n#         break","metadata":{"execution":{"iopub.status.busy":"2022-02-13T22:29:23.100491Z","iopub.status.idle":"2022-02-13T22:29:23.100916Z","shell.execute_reply.started":"2022-02-13T22:29:23.100666Z","shell.execute_reply":"2022-02-13T22:29:23.100688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# confis","metadata":{"execution":{"iopub.status.busy":"2022-02-13T22:29:23.102099Z","iopub.status.idle":"2022-02-13T22:29:23.102575Z","shell.execute_reply.started":"2022-02-13T22:29:23.102353Z","shell.execute_reply":"2022-02-13T22:29:23.102379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# demo = np.zeros((len(bboxes), 5))\n# demo[:,0] = confis\n# demo[:,1:5] = bboxes\n# demo","metadata":{"execution":{"iopub.status.busy":"2022-02-13T22:29:23.103869Z","iopub.status.idle":"2022-02-13T22:29:23.104431Z","shell.execute_reply.started":"2022-02-13T22:29:23.104194Z","shell.execute_reply":"2022-02-13T22:29:23.10422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}